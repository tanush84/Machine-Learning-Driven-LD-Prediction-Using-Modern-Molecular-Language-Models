{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7facf2-b27f-4683-ae95-63a7cc2059bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, classification_report, confusion_matrix\n",
    "CHEMBERT_NAME = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHEMBERT_NAME)\n",
    "config = AutoConfig.from_pretrained(CHEMBERT_NAME, output_hidden_states=True)\n",
    "encoder = AutoModel.from_pretrained(CHEMBERT_NAME, config=config)\n",
    "\n",
    "df_train = pd.read_csv('./NIHDataset/train_df_class.csv')\n",
    "df_test = pd.read_csv('./NIHDataset/test_df_class.csv')\n",
    "df_val = pd.read_csv('./NIHDataset/val_df_class.csv')\n",
    "\n",
    "df_train = df_train.rename(columns={\"LD50_class\": \"label\"})\n",
    "df_val   = df_val.rename(columns={\"LD50_class\": \"label\"})\n",
    "df_test  = df_test.rename(columns={\"LD50_class\": \"label\"})\n",
    "\n",
    "df_train = df_train[['smiles', 'label']].dropna()\n",
    "df_val   = df_val[['smiles', 'label']].dropna()\n",
    "df_test  = df_test[['smiles', 'label']].dropna()\n",
    "\n",
    "train_ds = Dataset.from_pandas(df_train.reset_index(drop=True))\n",
    "val_ds   = Dataset.from_pandas(df_val.reset_index(drop=True))\n",
    "test_ds  = Dataset.from_pandas(df_test.reset_index(drop=True))\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"smiles\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=160\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds   = val_ds.map(tokenize, batched=True)\n",
    "test_ds  = test_ds.map(tokenize, batched=True)\n",
    "train_ds = train_ds.rename_column(\"label\", \"labels\")\n",
    "val_ds   = val_ds.rename_column(\"label\", \"labels\")\n",
    "test_ds  = test_ds.rename_column(\"label\", \"labels\")\n",
    "\n",
    "cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_ds.set_format(\"torch\", columns=cols)\n",
    "val_ds.set_format(\"torch\", columns=cols)\n",
    "test_ds.set_format(\"torch\", columns=cols)\n",
    "def pool_hidden_states(outputs, attention_mask, mode=\"cls\"):\n",
    "    hidden_states = outputs.hidden_states\n",
    "\n",
    "    if mode == \"cls\":\n",
    "        return hidden_states[-1][:, 0]\n",
    "\n",
    "    elif mode == \"mean\":\n",
    "        last = hidden_states[-1]\n",
    "        mask = attention_mask.unsqueeze(-1)\n",
    "        return (last * mask).sum(1) / mask.sum(1)\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "class ChemBERTClassifier(nn.Module):\n",
    "    def __init__(self, encoder, num_classes, pooling=\"cls\"):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.pooling = pooling\n",
    "        self.classifier = ClassificationHead(\n",
    "            hidden_dim=encoder.config.hidden_size,\n",
    "            num_classes=num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        pooled = pool_hidden_states(outputs, attention_mask, self.pooling)\n",
    "        logits = self.classifier(pooled)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels.long())\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
    "        \"mcc\": matthews_corrcoef(labels, preds)\n",
    "    }\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./chembert_class\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"mcc\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "num_classes = len(df_train[\"label\"].unique())\n",
    "\n",
    "model = ChemBERTClassifier(\n",
    "    encoder=encoder,\n",
    "    num_classes=num_classes,\n",
    "    pooling=\"cls\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "outputs = trainer.predict(test_ds)\n",
    "\n",
    "logits = outputs.predictions\n",
    "labels = outputs.label_ids\n",
    "\n",
    "preds = np.argmax(logits, axis=1)\n",
    "\n",
    "print(\"\\n===== Test Results =====\")\n",
    "print(\"Accuracy:\", accuracy_score(labels, preds))\n",
    "print(\"MCC:\", matthews_corrcoef(labels, preds))\n",
    "print(\"F1:\", f1_score(labels, preds, average=\"macro\"))\n",
    "\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix(labels, preds))\n",
    "\n",
    "print(\"\\nClassification Report\")\n",
    "print(classification_report(labels, preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
